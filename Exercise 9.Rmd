---
title: '9'
author: "Sydney Jansen"
date: "2024-03-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
install.packages("skimr")
install.packages("ggpubr")
```


```{r}
#Step 1
#Using the {tidyverse} read_csv() function, load the “Street_et_al_2017.csv” dataset from this URL as a “tibble” named d.Do a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.

library(ggpubr)
library(tidyverse)
library(skimr)
library(dplyr)
library(tidyverse)
library(broom)
d <- read.csv("https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/Street_et_al_2017.csv")
as_tibble(d)
s <- skim(d)
summary <- s |> filter(skim_type == "numeric") |> rename(variable = skim_variable, missing = n_missing, mean = numeric.mean, sd = numeric.sd, min = numeric.p0, p25 = numeric.p25, median = numeric.p50, p75 = numeric.p75, max = numeric.p100) |> select(variable, missing, mean, sd, min, p25, median, p75, max)


```



```{r}
#Step 2
#From this dataset, plot brain size (ECV) as a function of social group size (Group_size), longevity (Longevity), juvenile period length (Weaning), and reproductive lifespan (Repro_lifespan).

library(ggplot2)
ggplot(data = d, aes(y = ECV, x = Group_size)) + geom_point()
ggplot(data = d, aes(y = ECV, x = Longevity)) + geom_point()
ggplot(data = d, aes(y = ECV, x = Weaning)) + geom_point()
ggplot(data = d, aes(y = ECV, x = Repro_lifespan)) + geom_point()
```

```{r}
# Step 3
#Derive by hand the ordinary least squares regression coefficients β1 and β0 for ECV as a function of social group size.
d <- d |>
  drop_na(ECV, Group_size)
beta1 <- cor(d$Group_size, d$ECV) * (sd(d$ECV)/sd(d$Group_size))
beta0 <- mean(d$ECV) - beta1 * mean(d$Group_size)

# Step 4
# Confirm that you get the same results using the `lm()` function.

m <- lm(ECV ~ Group_size, data = d)
summary(m)
```

```{r}
# Step 5
#Repeat the analysis above for three different major radiations of primates - “catarrhines”, “platyrrhines”, and “strepsirhines”) separately. These are stored in the variable Taxonomic_group. 
groups <- d |> filter_at(vars(ECV,Group_size),all_vars(!is.na(.))) |> group_by(Taxonomic_group) |> summarize(beta1 = (cor(Group_size, ECV) * (sd(ECV)/sd(Group_size))), beta0 = (mean(ECV) - beta1 * mean(Group_size)))
m <- lm(beta1 ~ beta0, data = groups)
summary(m)

#Do your regression coefficients differ among groups? How might you determine this?
#Yes, they differ widly. I determined this by runnning the summary with the new beta values



```
Step 6

For your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the p value associated with this coefficient by hand. Also extract this same information from the results of running the lm() function.
```{r}
SSY <- sum((m$model$ECV - mean(m$model$ECV))^2)
SSR <- sum((m$fitted.values - mean(m$model$ECV))^2)
SSE <- sum((m$model$ECV - m$fitted.values)^2)
MSR <- SSR/1 
MSR
MSE <- SSE/998 
MSE
MSY <- SSY/999 
MSY
fratio <- MSR/MSE
fratio
pf(q = fratio, df = 1, df2 = 998, lower.tail = FALSE)
SSX <- sum((m$model$Group_size - mean(m$model$Group_size))^2)
SEbeta1 <- sqrt(MSE/SSX)
SEbeta0 <- sqrt((MSE * sum(m$model$Group_size^2))/(151 * SSX))
SEbeta0

m.summary <- tidy(m)

percent_ci <- 95
alpha <- 1 - percent_ci / 100
t_crit <- qt(1 - alpha/2, df = nrow(d) - 2)


SEbeta1 <- m.summary$std.error[m.summary$term == "Group_size"]
beta1 <- m.summary$estimate[m.summary$term == "Group_size"]
lower_beta1 <- beta1 - t_crit * SEbeta1
upper_beta1 <- beta1 + t_crit * SEbeta1
CI_beta1 <- c(lower = lower_beta1, upper = upper_beta1)


SEbeta0 <- m.summary$std.error[m.summary$term == "(Intercept)"]
beta0 <- m.summary$estimate[m.summary$term == "(Intercept)"]
lower_beta0 <- beta0 - t_crit * SEbeta0
upper_beta0 <- beta0 + t_crit * SEbeta0
CI_beta0 <- c(lower = lower_beta0, upper = upper_beta0)


CI_beta1
CI_beta0

m <- lm(ECV ~ Group_size, data = d)
summary(m)




```
Step 7
Use a permutation approach with 1000 permutations to generate a null sampling distribution for the slope coefficient.

What is it that you need to permute? 
ECV and Group Size
What is the p value associated with your original slope coefficient? 
7.259435e-11
You can use either the percentile method (i.e., using quantiles from the actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error, along with a normal or t distribution), or both, to calculate this p value.
```{r}
library(lmodel2)
library(infer) #only thing to get "calculate" function to work
critical_value <- qt((1 - (alpha/2)), df = (149))
permuted.slope <- d |> specify(ECV ~ Group_size) |> hypothesize(null = "independence") |> generate(reps = 1000, type = "permute") |> calculate(stat = "slope")
permuted.slope.summary <- permuted.slope |> summarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value, perm.lower = quantile(stat, 0.025), perm.upper = quantile(stat, 0.975))
original.slope <- lm(data = d, ECV ~ Group_size) |> tidy(conf.int = TRUE, conf.level = 0.95) |> mutate(lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value) |> filter(term == "Group_size")
(p.value <- permuted.slope |> get_p_value(obs_stat = original.slope$estimate, direction="two_sided"))

```
Step 8
Use bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the percentile method and the theory-based method (i.e., using on the standard deviation of the bootstrapped sampling distribution as an estimate of the standard error). 
Do these CIs suggest that your slope coefficient is different from zero?
No, it is more than .05, therefore it is not significant
```{r}
library(infer)
boot.slope <- d |>
   specify(ECV ~ Group_size) |>
  generate(reps = 1000, type = "bootstrap") |>
   calculate(stat = "slope")

head(boot.slope)
boot.slope.summary <- boot.slope |> summarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value, boot.lower = quantile(stat, 0.025), boot.upper = quantile(stat, 0.975))
CI.percentile <- get_ci(boot.slope, level = 1 - alpha, type = "percentile")
CI.theory <- get_ci(boot.slope, level = 1 - alpha, type = "se", point_estimate = pull(boot.slope.summary,
    estimate))

t_stat <- t.test(x = boot.slope.summary, mu = boot.slope.summary$estimate, alternative = "greater")


```

